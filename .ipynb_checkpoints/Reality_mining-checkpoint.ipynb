{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reality Mining. Jupyter Notebook implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scipy library realitymining mat file is red and saved in to data variable.\n",
    "Since mat lab contains a lot of data, reading process take a lot of computer resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "data = scipy.io.loadmat(\"realitymining.mat\")['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affilation = data['my_affil']\n",
    "data_mat = data['data_mat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing.\n",
    "\n",
    "Extract __data_mat__ elements for each subject, categorize subjects to __sloan__ and __no sloan__ and save index of subjects who belongs 'sloan' category.\n",
    "<br>\n",
    "Also __frequency features and labels lists__ are created for Multi-layer Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sloan_list = []\n",
    "features_list = []\n",
    "frequency_list = []\n",
    "observation_days = 7\n",
    "count = 0\n",
    "frequency_labels = []\n",
    "\n",
    "for i in range(len(affilation[0])):\n",
    "    if len(data_mat[0][i]) > 0  and len(affilation[0][i]) > 0:\n",
    "        frequency_list += [data_mat[0][i]]\n",
    "        if affilation[0][i][0][0][0] == 'sloan' or affilation[0][i][0][0][0] == 'sloan_2':\n",
    "            frequency_labels += [1]\n",
    "        else:\n",
    "            frequency_labels += [0]\n",
    "        if len(data_mat[0][i][0]) >= observation_days:\n",
    "            features_list += [data_mat[0][i]]\n",
    "            if affilation[0][i][0][0][0] == 'sloan' or affilation[0][i][0][0][0] == 'sloan_2':\n",
    "                sloan_list += [count]\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude NaN values.\n",
    "Change all NaN values to value 4 in feature_list data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in range(len(features_list)):\n",
    "    for hour in range(len(features_list[subject])):\n",
    "        for element in range(len(features_list[subject][hour])):\n",
    "            if math.isnan(features_list[subject][hour][element]):\n",
    "                features_list[subject][hour][element] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Features vector for training and testing.\n",
    "<br>\n",
    "__observation data__ -  value of how many days subject activity is stored. By default we have one week - 7 days.\n",
    "<br>\n",
    "__all_places__ - it is a list of all possible places in data_set :\n",
    "\n",
    " Value | Explanation\n",
    "  -------------  | -------------\n",
    "  0 | No signal\n",
    "  1 | Home\n",
    "  2 | Work\n",
    "  3 | Elsewhere\n",
    "  4 | Phone is off\n",
    "<br>\n",
    "__count__ - Current index of element position in features vector.\n",
    "<br>\n",
    "__features__ - 2-rd dimensional numpy N-darray for storing features for RBM training:\n",
    "<br>\n",
    "* 1-st dimension - Subject\n",
    "* 2-nd dimension - Subject activity each hour of the week.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  0 – no signal, 1 – home, 2 – work, 3 – elsewhere, 4 – phone is off\n",
    "\n",
    "all_places = [0, 1, 2, 3, 4]\n",
    "count = 0\n",
    "\n",
    "features = np.zeros((len(features_list), observation_days * 24 * len(all_places)))\n",
    "\n",
    "for subject in range(len(features_list)):\n",
    "    for number in range(len(all_places)):\n",
    "        for week in range(observation_days):\n",
    "            for hours in range(24):\n",
    "                if features_list[subject][hours][week] == all_places[number]:\n",
    "                    features[subject][count] = 1\n",
    "                count += 1\n",
    "    count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_list = []  \n",
    "sk_rbm = BernoulliRBM(n_components=100, verbose=True, learning_rate=0.1, n_iter=1000)\n",
    "sk_rbm.fit(features)\n",
    "rbm_list = sk_rbm.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels creation for Multi-layer perceptron\n",
    "Each subject is classified to sloan or no_sloan.\n",
    "<br>\n",
    "If subject belongs to __sloan__ cateogy, his label value is equal __1__.\n",
    "<br>\n",
    "If subject belongs to __no sloan__ category, his label value is equal __0__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(features)):\n",
    "    if i in sloan_list:\n",
    "        labels += [1]\n",
    "    else:\n",
    "        labels += [0]\n",
    "        \n",
    "rbm_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency features creation for Multi-layer perceptron\n",
    "Frequencies of subjects calculated by summing locations categories for each type and sum dividing by all possible activities in the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 – home, 2 – work, 3 – elsewhere, 0 – no signal, 4 – phone is off\n",
    "home, work, elsewhere, no_signal, phone_off = 0, 0, 0, 0, 0\n",
    "frequency = []\n",
    "temp = []\n",
    "for subject in range(len(frequency_list)):\n",
    "    for hours in range(24):\n",
    "        for elements in range(len(frequency_list[subject][hours])):\n",
    "            if frequency_list[subject][hours][elements] == 1:\n",
    "                home += 1\n",
    "            elif frequency_list[subject][hours][elements] == 2:\n",
    "                work += 1\n",
    "            elif frequency_list[subject][hours][elements] == 3:\n",
    "                elsewhere += 1\n",
    "            elif frequency_list[subject][hours][elements] == 0:\n",
    "                no_signal += 1\n",
    "            else:\n",
    "                phone_off += 1\n",
    "        temp += [home/len(frequency_list[subject][hours]) if home !=0 else 0, \n",
    "                     work/len(frequency_list[subject][hours]) if  work !=0 else 0, \n",
    "                     elsewhere/len(frequency_list[subject][hours]) if elsewhere !=0 else 0, \n",
    "                     #no_signal/len(frequency_list[subject][hours]) if no_signal !=0 else 0,\n",
    "                     phone_off/len(frequency_list[subject][hours]) if phone_off !=0 else 0]\n",
    "        home, work, elsewhere, no_signal, phone_off = 0, 0, 0, 0, 0\n",
    "    frequency += [temp]\n",
    "    temp = []\n",
    "frequency_features = np.array(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM Neural Network implementation (second method)\n",
    "Splitting RBM features and labels data to train data for training and test data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, rbm_labels, test_size=0.3, random_state=0)\n",
    "rbm_list = []  \n",
    "sk_rbm = BernoulliRBM(n_components=100, verbose=True, learning_rate=0.1, n_iter=1000)\n",
    "sk_rbm.fit(X_train)\n",
    "X_train = sk_rbm.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters for MLP: \n",
    "* inputs - number of training features data.\n",
    "* n_hiddens - number of hidden neurons.\n",
    "* n_outputs - number of classifiers. This number is equal 2, because there are two labels: sloan and no_sloan.\n",
    "* n_epochs - Number of epochs which is a measure of the number of times all of the training vectors are used once to update the \t\t\t\t\t\t\tweights.\n",
    "* batch_size - defines number of samples that going to be propagated through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters that define the MLP\n",
    "n_inputs = len(X_train[0])\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 2\n",
    "X = tf.placeholder(tf.float32, shape= (None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implying activation function on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        # Number of inputs\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        # This value is computed to randomly initialize the weights\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        # Weigths can be initialized in different ways\n",
    "        # Here they are randomly initialized from a Normal distribution (mean=0,std as computed before)\n",
    "        # Notice that weights are organized in a matrix (tensor) and its number is n_inputs*n_neurons\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        # The variable that will contain the weights is W\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        \n",
    "        # The variable that will contain the bias is b  \n",
    "        # and is initialized to zero\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        \n",
    "        # As in the perceptron what the neurons do is multiply the weights by \n",
    "        # the input\n",
    "\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        \n",
    "        # What the activation function does is to \"process\" the result\n",
    "        # of the multiplication of weights by inputs, and this is the output\n",
    "        # of every neuron. \n",
    "    \n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define first and second layer using the __ReLu activation function__. Activation function is used to produce a non-linear decision boundary via non-linear combinations of the weighted inputs. __ReLu__ function is represented: __(X) = max(x,0)__ - if the input is greater than 0, the output is equal to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scope name for this MLP is \"dnn\"\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    # The first hidden layer is defined using the RELU activation function\n",
    "    # It will contain n_hidden1=300 hidden neurons and therefore output\n",
    "    # 300 values    \n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "\n",
    "    # The second hidden layer is also defined using the RELU activation function\n",
    "    # It will contain n_hidden2=100 hidden neurons and therefore output\n",
    "    # 100 values    \n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    \n",
    "    # The output layer does not use any activation function\n",
    "    # it will output n_outputs=10 values since there are 10 classes in MNIST\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define __loss function__. Loss function is a performance metric on how well the Neural Network manages to reach its goal of generating outputs as close as possible to the desired values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope name of the loss function is \"loss\"\n",
    "with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    # The Loss function is defined. It outputs a loss value for each x\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "#     xentropy = tf.contrib.keras.backend.binary_crossentropy(output=n_inputs, target = n_inputs, from_logits=False)\n",
    "    \n",
    "    # The total loss is mean of the loss values \n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing __Gradient Descent Optimizer__ which updates the weights towards less and less global loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # The plain GradientDescentOptimizer is chosen\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    # A prediction is correct if it is among the k=1 most probable\n",
    "    # classes predicted by the NN. Since k=1, it is only correct\n",
    "    # if the prediction coincides with the true class.\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    \n",
    "    # The accuracy is the mean of correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn weights using current batch and Compute accuracies in the training and validation sets using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialization of the computation graph\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# tensorflow allows to define a saver to store the model after learning\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# Size of the batch used to update the gradient\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(70 // batch_size):\n",
    "            \n",
    "            #Function next_batch automatically select the batch\n",
    "#             X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Weights are learned using the current batch            \n",
    "            sess.run(training_op, feed_dict={X: X_train, y: y_train})\n",
    "            \n",
    "        # Accuracies are computed in the training and validation sets    \n",
    "        acc_train = accuracy.eval(feed_dict={X:  X_train, y: y_train})\n",
    "        acc_val = accuracy.eval(feed_dict={X: sk_rbm.transform(X_test),\n",
    "                                            y: y_test})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  First method accuracy and training with Frequency MLP Neural networks using sklearn. \n",
    "Computes accuracy of frequently MLP for comparasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classificaton from RBM feature\n",
    "from sklearn import metrics\n",
    "X_train, X_test, y_train, y_test = train_test_split(frequency_features, frequency_labels, test_size=0.3, random_state=0)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "\n",
    "model = clf.fit(X_train, y_train)\n",
    "predicted_labels = model.predict(X_test)\n",
    "print(\"Accuracy %f\" % metrics.accuracy_score(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
